---
title: "Practical Machine Learning Assignment"
author: "John Robinson"
date: "17th November 2015"
output: html_document
---


## Introduction 

This report describes a data learning and prediction exercise performed as part of the Coursera Practical Machine Learning course. Data from a publicly available source were provided by the course supervisors as two files of results, a large training set, and a smaller test set. The data were investigated to create parsimonious sets of data, excluding any empty columns and non-numeric non predictive data such as identifiers.

These data were then investigated for fields with close to zero variation, and also for close correlations, before selecting a set of principal components with which to train the data modelling.

A second analysis was performed using all of the available covariates, in order to compare the accuracy of each approach.

In both models, validation was performed on the training data using three fold cross validation, and the best model used to predict the outcome of the test set, and to determine the expected out of sample error rate.

Finally a prediction was run on the test data set, using both models, and the results compared, before selecting which result is most likely to be correct.

## Background

Human Activity Recognition is a research area of growing interest among computer researchers. The availability of human activity data has increased dramatically with the sales of Smart Phones, Fitness Bands and other activity monitors.

In Veloso et. al (Reference 1), data on 'Qualitative Activity Recognition of Weight Lifting Exercises' are presented. The data used in this study has been made publicly available and forms the basis of this study. The work focussed on predicting how well a group of people performed a dumbbell lifting exercise.

## Data

Using wearable sensors, "Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E)." (reference 2)

The data collected in this research form the files provided for this machine learning exercise, where the object is to train a model using the training set data, and predict the class type of the results that form the test set.

#### Loading the data

The csv formatted files were downloaded to the local R-Studio project folder, and read into R as follows:

```{r cache=TRUE, warning=FALSE, message=FALSE}
TrainData <- read.csv("pml-Training.csv")
TestData <- read.csv("pml-Testing.csv")
```

#### Initial inspection

The resulting data frames contain 160 columns of information, with 19622 observations in the training set, and 20 in the test set. Such a large difference in sizes would not normally be appropriate, but for the purposes of briefness of submission data the test set was kept deliberately small.

An initial visual inspection of the data showed that many of the columns contain either blank or NA data. Inspecting the Test data alone these columns were seen to be completely empty of data, and therefore could not be used within a prediction model, so those columns were removed from both training and test data sets.

```{r cache=TRUE, warning=FALSE, message=FALSE}
TrainData <- TrainData[,colSums(is.na(TestData))==0]
TestData <- TestData[,colSums(is.na(TestData))==0]
```

This left a total of 60 columns, including the outcome data. 

On detailed inspection of the data it was apparent that the first seven columns contained only non-numeric, and non predictive values, so they were removed to provide two data sets with 52 possible predictors, and one outcome.

```{r cache=TRUE, warning=FALSE, message=FALSE}
TrainData <- TrainData[,8:60]
TestData <- TestData[,8:60]
```

#### Testing for near zero variation

Running the near zero option from the caret package reveals no columns with very low variation, indicating that all 52 may be suitable for model building.

```{r echo=FALSE, warning=FALSE, message=FALSE}
set.seed <- 2345
library(caret)
library(parallel)
```

```{r cache=TRUE, warning=FALSE, message=FALSE}
nearZeroVar(TrainData)
```

## Modelling

#### Selecting covariates

The data contain a large number of covariates. One option is to accept all as being important and build a suitable training model using these. This could be computationally intensive, so it may be worth investigating a smaller number to achieve an acceptable level of accuracy in the prediction.

Within the constraints of this project it is not possible to investigate each of the covariates individually, so a principal components approach has been used to determine the relationship between the number of covariates and the expected accuracy. The following graph shows the relationship between a threshold of variance description and the number of covariates required to account for that threshold.

```{r echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
j <- 0
x <- 1:13
y <- 1:13
for (i in c(0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.925,0.95,0.975,0.999999)){
        j=j+1
        x[j] <- i
        y[j] <- preProcess(TrainData[,-53],method = "pca",thresh = i)[12]
}
plot(x,y,col="red",xlab="Threshold",ylab="Covariates")
```

As the graph shows, in order to account for more than 90% of the variance the number of covariates required increases rapidly, suggesting that most, if not all, of the covariates are required for a truly accurate model.

On this basis it was decided to compare two sets of covariates, and to compare their out of sample errors with the number of covariates required.

#### Selecting a model type

As the data to be predicted form a categorical variable, with little information available to determine the basis upon which they were originally determined, the model cannot be formed by means of regression. Therefore some type of decision tree modelling is required. As the standard form of caret package training is a Random Forest approach, it was decided to use that.

#### Determining the expected level of accuracy

In order to determine out of sample error rates in the models, a cross validation technique using three-fold cross validation was selected. This can be built in to the training model using the caret packages training control options.

#### Model 1, using principal components

The first model was created using principal components with a threshold of covering 80% of the variation, resulting in a set of 12 covariates. The resulting model and its predicted out of sample error are shown here.

```{r cache=TRUE, warning=FALSE, message=FALSE}
preproc=preProcess(TrainData[,-53],method = "pca",thresh = 0.8)
preproc
trainPC=predict(preproc,TrainData[,-53])
modelpca=train(TrainData$classe ~.,data=trainPC,trControl=trainControl(method="cv",number=3,allowParallel = TRUE))
modelpca
modelpca$finalModel
```

As can be seen, the accuracy of the best model is estimated at 96%, with an out of sample error rate estimated at 2.71%.

#### Model 2, using all available covariates

The second model uses all 52 covariates, in a random forest model with three-fold cross validation. 

```{r cache=TRUE, warning=FALSE, message=FALSE}
trainingModel <- train(classe ~.,data=TrainData,trControl=trainControl(method="cv",number=3,allowParallel = TRUE))
trainingModel
trainingModel$finalModel
```

The accuracy of the best model is estimated at 99.3%, with an out of sample error rate estimated at 0.45%.

## Predictions

These two models were then applied to the 20 sets of data representing the test set. The 12 covariate principal component analysis prediction is:

```{r warning=FALSE, message=FALSE}
testPC=predict(preproc,TestData[,-53])
predictpca=predict(modelpca,testPC)
predictpca
```

The full 52 covariate analysis prediction is:

```{r warning=FALSE, message=FALSE}
prediction <- predict(trainingModel,newdata=TestData[,-53])
prediction
```

The differences can be seen with a confusion matrix

```{r, warning=FALSE, message=FALSE}
confusionMatrix(prediction,predictpca)$table
```

There is a difference in the prediction in two of the cases. It is assumed from the statistics of the models that the 52 covariate model is the more accurate.

## Conclusion

Two models were created, the first a 12 covariate principal component
analysis, and the second a full 52 covariate model. Each was used to train a predictive model using a Random Forest technique. Expected out of sample error rates were compared, showing that the full 52 covariate model was more accurate.

The final expected out of sample error rate, based on the 52 covariate model was 0.45%

The final prediction of the test data is:

```{r  warning=FALSE, message=FALSE}
prediction
```


## Reference

1. Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. 

2. http://groupware.les.inf.puc-rio.br/har#collaborators#ixzz3rJAkPebt


